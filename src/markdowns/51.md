---
layout: post
title: Badgeræºç å¯¼è¯»(ä¸€) - DBåˆå§‹åŒ–
date: 2022-12-04
tags: ["CFC æŠ€æœ¯"]
---

# Badgeræºç å¯¼è¯»

## æºç åˆ†æå…¥å£åŸºå‡†æ¡ˆä¾‹

å…ˆä»Badgerçš„åŸºæœ¬ä½¿ç”¨å…¥æ‰‹

    func main() {
        // æ‰“å¼€db
        db, _ := badger.Open(badger.DefaultOptions("tmp/badger"))
        defer db.Close()

        // è¯»å†™äº‹åŠ¡
        err := db.Update(func(txn *badger.Txn) error {
            txn.Set([]byte("answer"), []byte("42"))
            txn.Get([]byte("answer"))
            return nil
        })

        // åªè¯»äº‹åŠ¡
        err = db.View(func(txn *badger.Txn) error {
            txn.Get([]byte("answer_v1"))
            return nil
        })

        // éå†keys
        err = db.View(func(txn *badger.Txn) error {
            opts := badger.DefaultIteratorOptions
            opts.PrefetchSize = 10
            it := txn.NewIterator(opts)
            defer it.Close()
            for it.Rewind(); it.Valid(); it.Next() {
                item := it.Item()
                k := item.Key()
                err := item.Value(func(val []byte) error {
                    fmt.Printf("key=%s, value=%s\n", k, val)
                    return nil
                })
                if err != nil {
                    return err
                }
            }
            return nil
        })
        err = db.RunValueLogGC(0.7)
        _ = err
    }

## DBåˆå§‹åŒ–è¿‡ç¨‹

### åˆå§‹åŒ–å‚æ•°

`badger.open()`ä¼ å…¥çš„æ˜¯ä¸€ä¸ªoption,å…ˆçœ‹ä¸€ä¸‹optionç»“æ„ä½“çš„å­—æ®µéƒ½æœ‰å“ªäº›

    type Options struct {
        // Required options.

        // Dir: Badgeræ˜¯KVåˆ†ç¦»çš„å­˜å‚¨å¼•æ“,Dirä½ç½®å­˜å‚¨çš„æ˜¯ Key å’ŒæŒ‡å‘Valueçš„é€»è¾‘æŒ‡é’ˆ
        // ValueDir: å­˜å‚¨çš„æ˜¯Valueæ—¥å¿—,å³å€¼æ‰€åœ¨çš„åœ°å€,é»˜è®¤æƒ…å†µä¸‹Dirå’ŒValueDiråœ¨åŒä¸€ä¸ªpathç›®å½•ä¸‹
        Dir      string
        ValueDir string

        // Usually modified options.

        // SyncWrites: åŒæ­¥å†™,å³å†™å…¥çš„æ—¶å€™ä¸»åŠ¨åŒæ­¥åˆ°ç£ç›˜(mmapä¸ä¼šç«‹å³åˆ·ç›˜)
        SyncWrites        bool
        NumVersionsToKeep int
        // ReadOnly: å¦‚å…¶å,æ˜¯å¦è®¾ç½®ä¸ºåªè¯»
        ReadOnly          bool
        // Logger: å¦‚å…¶å,logå¯¹è±¡
        Logger            Logger
        // Compression: å‹ç¼©å½’å¹¶çš„çº§åˆ«
        Compression       options.CompressionType
        // InMemory: æ˜¯å¦åªåŸºäºå†…å­˜
        InMemory          bool
        MetricsEnabled    bool
        // Sets the Stream.numGo field
        NumGoroutines int

        // Fine tuning options.

        // MemTableSize: å†…å­˜è¡¨çš„å°ºå¯¸é™åˆ¶
        MemTableSize        int64
        BaseTableSize       int64
        BaseLevelSize       int64
        LevelSizeMultiplier int
        TableSizeMultiplier int
        // MaxLevels: æœ€å¤§å®¹å¿çš„levelçº§åˆ«,LSM-Tçš„çº§æ•°L0-L(max-1)
        MaxLevels           int

        VLogPercentile float64
        // ValueThreshold: å€¼å¤§å°çš„é˜ˆå€¼,å¦‚æœValueçš„å¤§å°ä¸è¶…è¿‡è¿™ä¸ªè®¾å®šå€¼,åˆ™ä¸ä¼šå°†KVè¿›è¡Œåˆ†ç¦»
        // æ­¤å¤„æ˜¯åœ¨å·¥ä¸šå®è·µä¸­çš„ä¸€ç§æƒè¡¡,KVåˆ†ç¦»ä¼šé€ æˆä¸å¯é¿å…çš„è¯»æ”¾å¤§
        // (ä¸¤æ¬¡çš„éšæœºè¯»,å…ˆåœ¨LSM-Tä¸­è¯»å–ä¸€æ¬¡æŒ‡é’ˆ,å†é€šè¿‡æŒ‡é’ˆä»ValueLogä¸­è¯»å–ä¸€æ¬¡å€¼)
        ValueThreshold int64
        // NumMemtables: å†…å­˜è¡¨çš„æ•°é‡
        NumMemtables   int
        // Changing BlockSize across DB runs will not break badger. The block size is
        // read from the block index stored at the end of the table.
        // BlockSize: æ¯ä¸ªblockçš„å¤§å°(sstç”±blockå’Œindexç­‰ç»„æˆ)
        BlockSize          int
        // BloomFalsePositive: å¸ƒéš†è¿‡æ»¤å™¨å‡é˜³æ€§çš„æ¯”ä¾‹
        BloomFalsePositive float64
        // BlockCacheSize: å—ç¼“å­˜çš„å¤§å°
        BlockCacheSize     int64
        // IndexCacheSize: ç´¢å¼•ç¼“å­˜çš„å¤§å°
        IndexCacheSize     int64

        NumLevelZeroTables      int
        NumLevelZeroTablesStall int

        // ValueLogFileSize: å­˜å‚¨å€¼çš„Valuelogæ–‡ä»¶çš„æœ€å¤§å¤§å°
        ValueLogFileSize   int64
        // ValueLogMaxEntries: å­˜å‚¨å€¼çš„Valuelogæ–‡ä»¶çš„æœ€å¤§é”®å€¼å¯¹æ•°é‡
        ValueLogMaxEntries uint32

        // NumCompactors: æ—¥å¿—åˆå¹¶å‹ç¼©åç¨‹åŒæ—¶è¿è¡Œçš„æœ€å¤§æ•°é‡
        NumCompactors        int
        CompactL0OnClose     bool
        LmaxCompaction       bool
        ZSTDCompressionLevel int

        // When set, checksum will be validated for each entry read from the value log file.
        // VerifyValueChecksum: æ˜¯å¦è¿›è¡Œå‚æ•°æ ¡éªŒå€¼çš„æ£€æŸ¥
        VerifyValueChecksum bool

        // Encryption related options.
        // EncryptionKey: åŠ å¯†å­—æ®µ 
        EncryptionKey                 []byte        // encryption key
        // EncryptionKeyRotationDuration: åŠ å¯†å­—æ®µæœ‰æ•ˆæ—¶é•¿
        EncryptionKeyRotationDuration time.Duration // key rotation duration

        // BypassLockGuard will bypass the lock guard on badger. Bypassing lock
        // guard can cause data corruption if multiple badger instances are using
        // the same directory. Use this options with caution.
        BypassLockGuard bool

        // ChecksumVerificationMode decides when db should verify checksums for SSTable blocks.
        ChecksumVerificationMode options.ChecksumVerificationMode

        // DetectConflicts determines whether the transactions would be checked for
        // conflicts. The transactions can be processed at a higher rate when
        // conflict detection is disabled.
        // DetectConflicts: äº‹åŠ¡çš„å†²çªæ£€æµ‹ 
        DetectConflicts bool

        // NamespaceOffset specifies the offset from where the next 8 bytes contains the namespace.
        NamespaceOffset int

        // Transaction start and commit timestamps are managed by end-user.
        // This is only useful for databases built on top of Badger (like Dgraph).
        // Not recommended for most users.
        managedTxns bool

        // 4. Flags for testing purposes
        // ------------------------------
        // æœ‰å…³æ‰¹å¤„ç†çš„å‚æ•°
        maxBatchCount int64 // max entries in batch
        maxBatchSize  int64 // max batch size in bytes

        maxValueThreshold float64
    }

ä¼ å…¥æŒ‡å®šçš„è·¯å¾„ï¼Œå¹¶é»˜è®¤é…ç½®ä¿¡æ¯ï¼Œå¦‚æœæœ‰éœ€è¦æ›´æ”¹çš„ä¿¡æ¯å¯ä»¥ä½¿ç”¨ `WithX()` æ–¹æ³•ï¼ˆæ­¤å¤„ä½¿ç”¨äº†`å»ºé€ è€…æ¨¡å¼`ï¼‰

`badger.DefaultOptions("tmp/badger")`

    // DefaultOptions sets a list of recommended options for good performance.
    // Feel free to modify these to suit your needs with the WithX methods.
    func DefaultOptions(path string) Options {
        return Options{

            Dir:      path,
            ValueDir: path,

            MemTableSize:        64 << 20,
            BaseTableSize:       2 << 20,
            BaseLevelSize:       10 << 20,
            TableSizeMultiplier: 2,
            LevelSizeMultiplier: 10,
            MaxLevels:           7,
            NumGoroutines:       8,
            MetricsEnabled:      true,

            NumCompactors:           4, // Run at least 2 compactors. Zero-th compactor prioritizes L0.
            NumLevelZeroTables:      5,
            NumLevelZeroTablesStall: 15,
            NumMemtables:            5,
            BloomFalsePositive:      0.01,
            BlockSize:               4 * 1024,
            SyncWrites:              false,
            NumVersionsToKeep:       1,
            CompactL0OnClose:        false,
            VerifyValueChecksum:     false,
            Compression:             options.Snappy,
            BlockCacheSize:          256 << 20,
            IndexCacheSize:          0,

            // The following benchmarks were done on a 4 KB block size (default block size). The
            // compression is ratio supposed to increase with increasing compression level but since the
            // input for compression algorithm is small (4 KB), we don't get significant benefit at
            // level 3.
            // NOTE: The benchmarks are with DataDog ZSTD that requires CGO. Hence, no longer valid.
            // no_compression-16              10     502848865 ns/op     165.46 MB/s    -
            // zstd_compression/level_1-16     7     739037966 ns/op     112.58 MB/s    2.93
            // zstd_compression/level_3-16     7     756950250 ns/op     109.91 MB/s    2.72
            // zstd_compression/level_15-16    1    11135686219 ns/op      7.47 MB/s    4.38
            // Benchmark code can be found in table/builder_test.go file
            ZSTDCompressionLevel: 1,

            // Nothing to read/write value log using standard File I/O
            // MemoryMap to mmap() the value log files
            // (2^30 - 1)*2 when mmapping < 2^31 - 1, max int32.
            // -1 so 2*ValueLogFileSize won't overflow on 32-bit systems.
            ValueLogFileSize: 1<<30 - 1,

            ValueLogMaxEntries: 1000000,

            VLogPercentile: 0.0,
            ValueThreshold: maxValueThreshold,

            Logger:                        defaultLogger(INFO),
            EncryptionKey:                 []byte{},
            EncryptionKeyRotationDuration: 10 * 24 * time.Hour, // Default 10 days.
            DetectConflicts:               true,
            NamespaceOffset:               -1,
        }
    }

### Openå‡½æ•°(æ ¸å¿ƒ)

`badger.Open(opt)` å‡½æ•°

> æ­¤æ–¹æ³•ä»£ç è¿‡é•¿,åœ¨æ­¤åªä¿ç•™æ ¸å¿ƒéƒ¨åˆ†ä»£ç ,éƒ¨åˆ†é€»è¾‘å°†ä»¥ä¼ªä»£ç æˆ–æ³¨é‡Šè¡¨ç¤º,å¹¶çœå»éƒ¨åˆ†é”™è¯¯å¤„ç†é€»è¾‘

    func Open(opt Options) (*DB, error) {

        // æ£€æŸ¥å‚æ•°
        checkAndSetOptions(&opt)

        // åˆ›å»ºäº†ä¸‰ä¸ªç›®å½•é”,é˜²æ­¢å…¶ä»–è¿›ç¨‹æ³¨å†Œåˆ°åŒä¸€ä¸ªç›®å½•é€ æˆå†²çª
        var dirLockGuard, valueDirLockGuard *directoryLockGuard

        // Create directories and acquire lock on it only if badger is not running in InMemory mode.
        // We don't have any directories/files in InMemory mode so we don't need to acquire
        // any locks on them.
        // åˆ¤æ–­å‚æ•°é…ç½®ä¸ºåªåŸºäºå†…å­˜
        if !opt.InMemory {
            // åˆ›å»ºç›®å½•
            createDirs(opt)

            var err error
            if !opt.BypassLockGuard {
                // ç»™DiråŠ ç›®å½•é”
                dirLockGuard, _ = acquireDirectoryLock(opt.Dir, lockFile, opt.ReadOnly)
                // æ–¹æ³•æœ«å°¾é‡Šæ”¾é”
                defer func() {
                    if dirLockGuard != nil {
                        _ = dirLockGuard.release()
                    }
                }()
                // è·å–Key&ValuePtrçš„ç»å¯¹è·¯å¾„
                absDir, _ := filepath.Abs(opt.Dir)

                // è·å–ValueLogçš„ç»å¯¹è·¯å¾„
                absValueDir, _ := filepath.Abs(opt.ValueDir)

                // å¦‚æœValueDirå’ŒDirä¸ç›¸åŒ,éœ€è¦å„è‡ªåŠ é”
                if absValueDir != absDir {
                    // ç»™ValueDiråŠ ç›®å½•é”
                    valueDirLockGuard, _ = acquireDirectoryLock(opt.ValueDir, lockFile, opt.ReadOnly)

                    // é‡Šæ”¾é”
                    defer func() {
                        if valueDirLockGuard != nil {
                            _ = valueDirLockGuard.release()
                        }
                    }()
                }
            }
        }

        // æ‰“å¼€æˆ–åˆ›å»ºManifestæ–‡ä»¶,(é‡‡ç”¨mmapæ–¹å¼æ‰“å¼€,åœ¨åé¢è¯¦ç»†å±•å¼€)
        manifestFile, manifest, _ := openOrCreateManifestFile(opt)

        // å…³é—­Manifestæ–‡ä»¶
        defer func() {
            if manifestFile != nil {
                _ = manifestFile.close()
            }
        }()

        // åˆ›å»ºå†…å­˜ä¸­çš„dbæ•°æ®ç»“æ„
        db := &DB{
            // memtable, å› ä¸ºæœ‰å¤šä¸ª,æ‰€ä»¥è¦åˆ›å»ºæ•°ç»„
            imm:              make([]*memTable, 0, opt.NumMemtables),
            // åˆ·æ–°è¯·æ±‚çš„channel
            flushChan:        make(chan flushTask, opt.NumMemtables),
            // å†™è¯·æ±‚çš„channel
            writeCh:          make(chan *request, kvWriteChCapacity),
            // é…ç½®ä¿¡æ¯opt
            opt:              opt,
            // åˆšåˆå§‹åŒ–å¥½çš„manifestå®ä¾‹
            manifest:         manifestFile,
            // Key&ValuePtrç›®å½•é”
            dirLockGuard:     dirLockGuard,
            // Valueç›®å½•é”
            valueDirGuard:    valueDirLockGuard,
            // Oracleçš„å®ä¾‹,ä¸€ä¸ªKVå¼•æ“å¹¶å‘äº‹åŠ¡çš„ç®¡ç†å™¨,è´Ÿè´£åˆ†é…äº‹åŠ¡çš„ç‰ˆæœ¬å·,ç”¨æ¥å®ç°MVCCåŠŸèƒ½,åœ¨è¯»å†™äº‹åŠ¡æ—¶è¯¦ç»†å±•å¼€
            orc:              newOracle(opt),
            pub:              newPublisher(),
            allocPool:        z.NewAllocatorPool(8),
            bannedNamespaces: &lockedKeys{keys: make(map[uint64]struct{})},
            threshold:        initVlogThreshold(&opt),
        }
        // Cleanup all the goroutines started by badger in case of an error.
        // å…³é—­badgerçš„æ‰€æœ‰ä»»åŠ¡åç¨‹çš„é’©å­å‡½æ•°
        defer func() {
            if err != nil {
                opt.Errorf("Received err: %v. Cleaning up...", err)
                db.cleanup()
                db = nil
            }
        }()

        // å—ç¼“å­˜ç›¸å…³é…ç½®
        // LSM-Tç»“æ„ä¸­SSTé‡Œé¢æ•°æ®æ˜¯ä»¥å—(block)ä¸ºå•ä½åˆ†å‰²çš„
        // å½“å¼€å¯å—ç¼“å­˜ä¹‹å,LSM-Tä¼šæŠŠæœ€è¿‘è¢«è®¿é—®åˆ°çš„é«˜çƒ­çš„å—ç¼“å­˜åœ¨å†…å­˜ä¸­,ä»¥åŠ å—å“åº”é€Ÿåº¦
        if opt.BlockCacheSize > 0 {
            // ç¼“å­˜ä¸åœ¨æ­¤æ¬¡æºç é˜…è¯»çš„è®¨è®ºèŒƒå›´ä¹‹å†…,ä¸å½±å“æ ¸å¿ƒåŠŸèƒ½,æš‚ä¸”ç•¥è¿‡
            // å€¼å¾—ä¸€æçš„æ˜¯badgeræ˜¯ä½¿ç”¨çš„ç¼“å­˜æ˜¯badgerç¤¾åŒºç ”å‘çš„ä¸€ä¸ªé«˜æ€§èƒ½æœ¬åœ°å¹¶å‘ç¼“å­˜çš„åº“,æœ‰å…´è¶£çš„åŒå­¦å¯ä»¥è‡ªè¡Œç ”ç©¶
            numInCache := opt.BlockCacheSize / int64(opt.BlockSize)
            if numInCache == 0 {
                // Make the value of this variable at least one since the cache requires
                // the number of counters to be greater than zero.
                numInCache = 1
            }

            config := ristretto.Config{
                NumCounters: numInCache * 8,
                MaxCost:     opt.BlockCacheSize,
                BufferItems: 64,
                Metrics:     true,
                OnExit:      table.BlockEvictHandler,
            }
            db.blockCache, err = ristretto.NewCache(&config)
            if err != nil {
                return nil, y.Wrap(err, "failed to create data cache")
            }
        }

        // ç´¢å¼•ç¼“å­˜ç›¸å…³é…ç½®
        // ç´¢å¼•æ˜¯æ¯ä¸ªKeyæ‰€å¯¹åº”çš„åç¦»é‡çš„å€¼,æ¯ä¸€ä¸ªSSTableæœ‰ä¸€ä¸ªå…ƒæ•°æ®å—å³ç´¢å¼•å—
        // å¯ä»¥æ–¹ä¾¿å¯¹Keyçš„äºŒåˆ†æŸ¥æ‰¾,å®šä½å½“å‰çš„keyåœ¨å“ªä¸€ä¸ªsstableæ–‡ä»¶é‡Œ,åœ¨æ–‡ä»¶ä¸­çš„åç§»é‡æ˜¯å¤šå°‘
        if opt.IndexCacheSize > 0 {
            // Index size is around 5% of the table size.
            indexSz := int64(float64(opt.MemTableSize) * 0.05)
            numInCache := opt.IndexCacheSize / indexSz
            if numInCache == 0 {
                // Make the value of this variable at least one since the cache requires
                // the number of counters to be greater than zero.
                numInCache = 1
            }

            config := ristretto.Config{
                NumCounters: numInCache * 8,
                MaxCost:     opt.IndexCacheSize,
                BufferItems: 64,
                Metrics:     true,
            }
            db.indexCache, err = ristretto.NewCache(&config)
            if err != nil {
                return nil, y.Wrap(err, "failed to create bf cache")
            }
        }

        // å¯¹ç¼“å­˜æ¨¡å—çš„ç›‘æ§æ£€æµ‹
        db.closers.cacheHealth = z.NewCloser(1)
        go db.monitorCache(db.closers.cacheHealth)

        // å¦‚æœä»…åŸºäºå†…å­˜
        if db.opt.InMemory {
            // é»˜è®¤å…³é—­å†™åŒæ­¥
            db.opt.SyncWrites = false
            // If badger is running in memory mode, push everything into the LSM Tree.
            // æŠŠæ‰€æœ‰æ•°æ®åªå†™åœ¨LSM-Tä¸­
            db.opt.ValueThreshold = math.MaxInt32
        }

        // Keyçš„æ³¨å†Œ,ä¸å¹¶å‘äº‹åŠ¡ç›¸å…³,ä¹‹åå†è¯¦ç»†å±•å¼€
        krOpt := KeyRegistryOptions{
            ReadOnly:                      opt.ReadOnly,
            Dir:                           opt.Dir,
            EncryptionKey:                 opt.EncryptionKey,
            EncryptionKeyRotationDuration: opt.EncryptionKeyRotationDuration,
            InMemory:                      opt.InMemory,
        }
        db.registry, _ = OpenKeyRegistry(krOpt)

        // è®¡ç®—æ¶ˆè€—çš„å†…å­˜ç­‰æ•°æ®ç»Ÿè®¡ä¿¡æ¯
        db.calculateSize()
        db.closers.updateSize = z.NewCloser(1)
        go db.updateSize(db.closers.updateSize)

        // æ‰“å¼€ä¸€ä¸ªmemTableå®ä¾‹
        // memtableæ˜¯åœ¨å†…å­˜ä¸­çš„ä¸€ä¸ªå¤æ‚æ•°æ®ç»“æ„
        if err := db.openMemTables(db.opt); err != nil {
            return nil, y.Wrapf(err, "while opening memtables")
        }
        // æ£€æŸ¥
        if !db.opt.ReadOnly {
            // åˆ›å»ºä¸€ä¸ªæ–°çš„.memæ–‡ä»¶
            // .memæ–‡ä»¶å°±æ˜¯LSM-Tä¸­çš„é¢„å†™æ—¥å¿—æ–‡ä»¶(wal)
            if db.mt, err = db.newMemTable(); err != nil {
                return nil, y.Wrapf(err, "cannot create memtable")
            }
        }

        // newLevelsController potentially loads files in directory.
        // åˆ›å»ºå†…å­˜ä¸­levelç®¡ç†å™¨
        // LSM-Tæ˜¯åˆ†å±‚ç»“æ„çš„, LevelsControllerå®ä¾‹è´Ÿè´£ç»´æŠ¤æ•´ä¸ªå±‚çº§ç»“æ„
        // è¿›è¡Œæ—¥å¿—å½’å¹¶,å‹ç¼©å¤„ç†ç­‰æ“ä½œ,é€šè¿‡Manifestè¿›è¡Œåˆå§‹é…ç½®
        // æˆ–è€…æ˜¯,manifestæ–‡ä»¶å°±æ˜¯LevelControlleræŒä¹…åŒ–ä¹‹åçš„ondiskç‰ˆæœ¬,å¯ä»¥åŠ å¿«badgerçš„æ¢å¤é‡å¯é€Ÿåº¦
        // å…ˆæ‰“å¼€SSTable,åŠ è½½ç´¢å¼•å—,å…ƒæ•°æ®å—,ç¼“å­˜åˆ°å†…å­˜å½“ä¸­
        if db.lc, err = newLevelsController(db, &manifest); err != nil {
            return db, err
        }

        // Initialize vlog struct.
        // åˆå§‹åŒ–vlog
        db.vlog.init(db)

        if !opt.ReadOnly {
            // å¯åŠ¨æ—¥å¿—å½’å¹¶çš„å·¥ä½œåç¨‹,åç»­å†å±•å¼€
            db.closers.compactors = z.NewCloser(1)
            db.lc.startCompact(db.closers.compactors)

            db.closers.memtable = z.NewCloser(1)
            go func() {
                _ = db.flushMemtable(db.closers.memtable) // Need levels controller to be up.
            }()
            // Flush them to disk asap.
            for _, mt := range db.imm {
                db.flushChan <- flushTask{mt: mt}
            }
        }
        // We do increment nextTxnTs below. So, no need to do it here.
        // æ‹¿åˆ°å¯åŠ¨æ—¶æœ€å¤§äº‹åŠ¡çš„ç‰ˆæœ¬å·(æ—¶é—´æˆ³)
        db.orc.nextTxnTs = db.MaxVersion()
        db.opt.Infof("Set nextTxnTs to %d", db.orc.nextTxnTs)

        // çœŸæ­£æ‰“å¼€vlogæ–‡ä»¶
        if err = db.vlog.open(db); err != nil {
            return db, y.Wrapf(err, "During db.vlog.open")
        }

        // Let's advance nextTxnTs to one more than whatever we observed via
        // replaying the logs.
        // äº‹åŠ¡ç›¸å…³,ç­‰å¾…ä¹‹å‰äº‹åŠ¡çš„æ¢å¤
        db.orc.txnMark.Done(db.orc.nextTxnTs)
        // In normal mode, we must update readMark so older versions of keys can be removed during
        // compaction when run in offline mode via the flatten tool.
        db.orc.readMark.Done(db.orc.nextTxnTs)
        // äº‹åŠ¡å·è‡ªå¢
        db.orc.incrementNextTs()

        // ç›‘å¬é…ç½®ä¿¡æ¯çš„æ›´æ”¹
        go db.threshold.listenForValueThresholdUpdate()

        // ä»æ•°æ®åº“ä¸­æ£€ç´¢è¢«ç¦æ­¢çš„å‘½åç©ºé—´å¹¶æ›´æ–°å†…å­˜ç»“æ„(éé‡ç‚¹)
        if err := db.initBannedNamespaces(); err != nil {
            return db, errors.Wrapf(err, "While setting banned keys")
        }

        // å¯åŠ¨å¤„ç†ç£ç›˜å†™è¯·æ±‚çš„åç¨‹
        // badgerçš„å†™ä»»åŠ¡æ˜¯å¹¶å‘å†™ä»»åŠ¡,å¯ä»¥å……åˆ†å‘æŒ¥ssdçš„æ€§èƒ½
        db.closers.writes = z.NewCloser(1)
        go db.doWrites(db.closers.writes)

        if !db.opt.InMemory {
            // çœŸæ­£å¼€å¯vlogçš„GC, åé¢å†è¯¦ç»†è®²è§£
            db.closers.valueGC = z.NewCloser(1)
            go db.vlog.waitOnGC(db.closers.valueGC)
        }

        // ç›‘å¬åç¨‹(éé‡ç‚¹)
        db.closers.pub = z.NewCloser(1)
        go db.pub.listenForUpdates(db.closers.pub)

        // é‡Šæ”¾é”
        valueDirLockGuard = nil
        dirLockGuard = nil
        manifestFile = nil
        // è¿”å›db
        return db, nil
    }

#### åˆ›å»ºManifestæ–‡ä»¶

`openOrCreateManifestFile(opt)` å‡½æ•°

    func openOrCreateManifestFile(opt Options) (ret *manifestFile, result Manifest, err error) {
        // å¦‚æœInmemoryåˆ™è¿”å›ç©ºçš„Manifest
        if opt.InMemory {
            return &manifestFile{inMemory: true}, Manifest{}, nil
        }
        return helpOpenOrCreateManifestFile(opt.Dir, opt.ReadOnly, manifestDeletionsRewriteThreshold)
    }

    func helpOpenOrCreateManifestFile(dir string, readOnly bool, deletionsThreshold int) (*manifestFile, Manifest, error) {
        // æ‹¼æ¥path
        path := filepath.Join(dir, ManifestFilename)
        var flags y.Flags
        if readOnly {
            flags '= y.ReadOnly
        }
        // å°è¯•æ‰“å¼€æ–‡ä»¶
        fp, err := y.OpenExistingFile(path, flags) // We explicitly sync in addChanges, outside the lock.
        if err != nil {
            // æ ¡éªŒæ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if !os.IsNotExist(err) {
                return nil, Manifest{}, err
            }
            // å¦‚æœä»…è¯»åˆ™æ— æ³•åˆ›å»ºç›´æ¥è¿”å›
            if readOnly {
                return nil, Manifest{}, fmt.Errorf("no manifest found, required for read-only db")
            }
            // çœŸæ­£åˆ›å»ºmanifestå®ä¾‹
            m := createManifest()
            // è¦†ç›–å†™,æ‰§è¡Œå®Œæ­¤æ¡è¯­å¥åå°±å¯ä»¥åœ¨ç›®å½•ä¸­çœ‹åˆ°MANIFESTæ–‡ä»¶å­˜åœ¨äº†(æ­¤æ—¶MANIFESTæ–‡ä»¶ä¸­ä»…æœ‰é­”æ•°bdg)
            fp, netCreations, _ := helpRewrite(dir, &m)

            // æ–­è¨€,ç¡®ä¿åˆ›å»ºæˆåŠŸ
            y.AssertTrue(netCreations == 0)
            // åˆ›å»ºmanifestFileå®ä¾‹åœ¨å†…å­˜ä¸­ä¿å­˜ä¿¡æ¯
            mf := &manifestFile{
                fp:                        fp,
                directory:                 dir,
                manifest:                  m.clone(),
                deletionsRewriteThreshold: deletionsThreshold,
            }
            return mf, m, nil
        }

        // æ–‡ä»¶å­˜åœ¨åŠ è½½æ¢å¤çš„é€»è¾‘æš‚ä¸å±•å¼€
        ......
    }

    func createManifest() Manifest {
        levels := make([]levelManifest, 0)
        return Manifest{
            Levels: levels,
            Tables: make(map[uint64]TableManifest),
        }
        // Tables: map[uint64]TableManifest
        // uint64: è¡Œå·,ç¬¬nä¸ªlevel
    }

#### æ‰“å¼€Memtable

`memtable` ç»“æ„ä½“

    // memTable structure stores a skiplist and a corresponding WAL. Writes to memTable are written
    // both to the WAL and the skiplist. On a crash, the WAL is replayed to bring the skiplist back to
    // its pre-crash form.
    type memTable struct {
        sl         *skl.Skiplist
        wal        *logFile
        maxVersion uint64
        opt        Options
        buf        *bytes.Buffer
    }

`openMemTables(opt)` æ–¹æ³•

    func (db *DB) openMemTables(opt Options) error {
        // We don't need to open any tables in in-memory mode.
        // å¦‚æœæ˜¯åªåŸºäºå†…å­˜åˆ™ç›´æ¥è¿”å›(é‚£æˆ‘èµ°?)
        if db.opt.InMemory {
            return nil
        }
        // è¯»å–ç›®å½•ä¸­çš„å…¨éƒ¨æ–‡ä»¶
        files, _ := ioutil.ReadDir(db.opt.Dir)

        var fids []int
        // éå†ç›®å½•ä¸­çš„æ–‡ä»¶
        for _, file := range files {
            // æ£€æŸ¥å½“å‰æ–‡ä»¶åæ˜¯å¦åŒ…å«ä¸€ä¸ª.memçš„åç¼€(åœ¨ç¬¬ä¸€æ¬¡åˆå§‹åŒ–è¿‡ç¨‹ä¸­è‚¯å®šä¸ä¼šå­˜åœ¨)
            // æ­¤æ—¶ç›®å½•ä¸­åº”æœ‰çš„æ–‡ä»¶ä¸º LOCK MANIFEST KEYREGISTRY
            if !strings.HasSuffix(file.Name(), memFileExt) {
                continue
            }
            // å¦‚æœæœ‰.memæ–‡ä»¶,åˆ™å–æ–‡ä»¶çš„å‘½åè½¬ä¸ºintå€¼ä½œä¸ºfid
            // ä¾‹: 000001.mem 000002.mem
            fsz := len(file.Name())
            fid, _ := strconv.ParseInt(file.Name()[:fsz-len(memFileExt)], 10, 64)

            fids = append(fids, int(fid))
        }

        // Sort in ascending order.
        // æŒ‰ç…§fidæ’åº
        sort.Slice(fids, func(i, j int) bool {
            return fids[i] < fids[j]
        })
        // æŒ‰ç…§fidé¡ºåºéå†
        for _, fid := range fids {
            flags := os.O_RDWR
            if db.opt.ReadOnly {
                flags = os.O_RDONLY
            }
            // çœŸæ­£çš„æ‰“å¼€.memæ–‡ä»¶,é‡‡ç”¨mmapæ–¹å¼åŠ è½½.memæ–‡ä»¶ä¸­çš„æ•°æ®
            mt, err := db.openMemTable(fid, flags)
            if err != nil {
                return y.Wrapf(err, "while opening fid: %d", fid)
            }
            // If this memtable is empty we don't need to add it. This is a
            // memtable that was completely truncated.
            if mt.sl.Empty() {
                mt.DecrRef()
                continue
            }
            // These should no longer be written to. So, make them part of the imm.
            db.imm = append(db.imm, mt)
        }
        // è®¾ç½®æœ€æ–°çš„fidåºåˆ—å·
        if len(fids) != 0 {
            db.nextMemFid = fids[len(fids)-1]
        }
        db.nextMemFid++
        return nil
    }

#### åˆ›å»ºMemtable

`newMemTable()` æ–¹æ³•

    func (db *DB) newMemTable() (*memTable, error) {
        // çœŸæ­£åˆ›å»º.memæ–‡ä»¶
        mt, err := db.openMemTable(db.nextMemFid, os.O_CREATE'os.O_RDWR)
        if err == z.NewFile {
            db.nextMemFid++
            return mt, nil
        }

        if err != nil {
            db.opt.Errorf("Got error: %v for id: %d\n", err, db.nextMemFid)
            return nil, y.Wrapf(err, "newMemTable")
        }
        return nil, errors.Errorf("File %s already exists", mt.wal.Fd.Name())
    }

`openMemTable(fid, flags)` æ–¹æ³•

    func (db *DB) openMemTable(fid, flags int) (*memTable, error) {
        // æ‹¼æ¥è·¯å¾„
        filepath := db.mtFilePath(fid)
        // åˆ›å»ºmemtableä¸­çš„skiplist
        s := skl.NewSkiplist(arenaSize(db.opt))
        // åˆ›å»ºmemtableå®ä¾‹
        mt := &memTable{
            sl:  s,
            opt: db.opt,
            buf: &bytes.Buffer{},
        }
        // We don't need to create the wal for the skiplist in in-memory mode so return the mt.
        // å¦‚æœåªåŸºäºå†…å­˜,åˆ™ä¸éœ€è¦åˆ›å»ºwalæ–‡ä»¶,ç›´æ¥è¿”å›
        if db.opt.InMemory {
            return mt, z.NewFile
        }
        // åˆ›å»ºwalæ–‡ä»¶å®ä¾‹
        mt.wal = &logFile{
            fid:      uint32(fid),
            path:     filepath,
            registry: db.registry,
            writeAt:  vlogHeaderSize,
            opt:      db.opt,
        }
        // è°ƒç”¨ç³»ç»Ÿå‡½æ•°åˆ›å»ºwalæ–‡ä»¶
        lerr := mt.wal.open(filepath, flags, 2*db.opt.MemTableSize)
        // å¦‚æœæœªæˆåŠŸåˆ›å»ºæ–°æ–‡ä»¶æˆ–å…¶ä»–å¤±è´¥åˆ™è¿”å›err
        if lerr != z.NewFile && lerr != nil {
            return nil, y.Wrapf(lerr, "While opening memtable: %s", filepath)
        }

        // Have a callback set to delete WAL when skiplist reference count goes down to zero. That is,
        // when it gets flushed to L0.
        // ç”¨æ¥å…³é—­çš„å›è°ƒå‡½æ•°
        s.OnClose = func() {
            if err := mt.wal.Delete(); err != nil {
                db.opt.Errorf("while deleting file: %s, err: %v", filepath, err)
            }
        }
        // æˆåŠŸåˆ›å»ºmmapåˆ™è¿”å› lerr (z.NewFile)
        if lerr == z.NewFile {
            return mt, lerr
        }
        // å½“ä¸”ä»…å½“MemTableSizeè®¾ç½®ä¸º0æ—¶é€ æˆ lerr == nilçš„é€‚åˆæ‰§è¡Œåˆ°æ­¤
        // æ­¤æ—¶mmapæœªè¿›è¡Œæˆªæ–­,åœ¨UpdateSkipList()ä¸­éå†walæ–‡ä»¶å¹¶é‡æ–°æˆªæ–­,å¦‚æœwalæ–‡ä»¶ä¸å­˜åœ¨ä¼šè¿”å›é”™è¯¯
        err := mt.UpdateSkipList()
        return mt, y.Wrapf(err, "while updating skiplist")
    }

#### åˆ›å»ºlevelController

`newLevelsController(db, mf)` å‡½æ•°

    func newLevelsController(db *DB, mf *Manifest) (*levelsController, error) {
        // æ–­è¨€,è¿›è¡Œä¸€äº›å¿…è¦çš„æ ¡éªŒ
        y.AssertTrue(db.opt.NumLevelZeroTablesStall > db.opt.NumLevelZeroTables)
        // å…³è”dbå®ä¾‹,åˆ›å»ºlevelæ•°ç»„å¯¹åº”å±‚çº§å…³ç³»(ä¾‹:levels[0] => L0å±‚)
        // levelHandlerå°±æ˜¯çœŸæ­£è´Ÿè´£æŸä¸€å±‚sstç®¡ç†å™¨çš„ä¸»è¦æ“ä½œ
        s := &levelsController{
            kv:     db,
            levels: make([]*levelHandler, db.opt.MaxLevels),
        }
        // çŠ¶æ€ç»Ÿè®¡çš„ä¸€ä¸ªçš„å¯¹è±¡(setç»“æ„),keyä¸ºfid,ç”¨ä»¥åˆ¤æ–­å¯¹åº”çš„fidæ˜¯å¦å­˜åœ¨äºè¿™ä¸€å±‚
        s.cstatus.tables = make(map[uint64]struct{})
        // åˆå¹¶çŠ¶æ€çš„ä¿¡æ¯
        s.cstatus.levels = make([]*levelCompactStatus, db.opt.MaxLevels)

        // æŒ‰å±‚éå†,æ¯ä¸€å±‚éƒ½åˆ›å»ºä¸€ä¸ªlevelhandlerå®ä¾‹
        for i := 0; i < db.opt.MaxLevels; i++ {
            s.levels[i] = newLevelHandler(db, i)
            s.cstatus.levels[i] = new(levelCompactStatus)
        }
        // åŸºäºå†…å­˜,é‚£æˆ‘èµ°?ğŸ¤¡
        if db.opt.InMemory {
            return s, nil
        }
        // Compare manifest against directory, check for existent/non-existent files, and remove.
        // å¯¹manifestæ–‡ä»¶è¿›è¡Œæ ¡éªŒ
        if err := revertToManifest(db, mf, getIDMap(db.opt.Dir)); err != nil {
            return nil, err
        }

        var mu sync.Mutex
        tables := make([][]*table.Table, db.opt.MaxLevels)
        var maxFileID uint64

        // We found that using 3 goroutines allows disk throughput to be utilized to its max.
        // Disk utilization is the main thing we should focus on, while trying to read the data. That's
        // the one factor that remains constant between HDD and SSD.
        // ä¸€ç§é’ˆå¯¹å¹¶å‘æ§åˆ¶çš„è´Ÿè½½å‡è¡¡ç­–ç•¥,å¯¹äºssdæ¥è¯´,åˆ›å»º3ä¸ªåç¨‹èƒ½å¤Ÿæœ€å¤§çš„å‘æŒ¥ssdçš„ä¼˜ç‚¹
        throttle := y.NewThrottle(3)

        start := time.Now()
        var numOpened int32
        // åˆ›å»ºä¸€ä¸ªå®šæ—¶è§¦å‘å™¨è¿›è¡Œè¶…æ—¶æ§åˆ¶
        tick := time.NewTicker(3 * time.Second)
        // é’©å­å‡½æ•°å…³é—­å®šæ—¶å™¨
        defer tick.Stop()

        // manifestæ¸…å•æ–‡ä»¶çš„Tables
        // æ‹¿åˆ°æ¯ä¸ªtableå¯¹åº”çš„fid
        // ç¬¬ä¸€æ¬¡åˆå§‹åŒ–çš„é€‚åˆå› ä¸ºTablesä¸ºç©º,ä¼šç›´æ¥è·³è¿‡
        for fileID, tf := range mf.Tables {
            fname := table.NewFilename(fileID, db.opt.Dir)
            select {
            case <-tick.C:
                db.opt.Infof("%d tables out of %d opened in %s\n", atomic.LoadInt32(&numOpened),
                    len(mf.Tables), time.Since(start).Round(time.Millisecond))
            default:
            }
            if err := throttle.Do(); err != nil {
                closeAllTables(tables)
                return nil, err
            }
            if fileID > maxFileID {
                maxFileID = fileID
            }
            go func(fname string, tf TableManifest) {
                var rerr error
                defer func() {
                    throttle.Done(rerr)
                    atomic.AddInt32(&numOpened, 1)
                }()
                dk, err := db.registry.DataKey(tf.KeyID)
                if err != nil {
                    rerr = y.Wrapf(err, "Error while reading datakey")
                    return
                }
                topt := buildTableOptions(db)
                // Explicitly set Compression and DataKey based on how the table was generated.
                topt.Compression = tf.Compression
                topt.DataKey = dk

                mf, err := z.OpenMmapFile(fname, db.opt.getFileFlags(), 0)
                if err != nil {
                    rerr = y.Wrapf(err, "Opening file: %q", fname)
                    return
                }
                t, err := table.OpenTable(mf, topt)
                if err != nil {
                    if strings.HasPrefix(err.Error(), "CHECKSUM_MISMATCH:") {
                        db.opt.Errorf(err.Error())
                        db.opt.Errorf("Ignoring table %s", mf.Fd.Name())
                        // Do not set rerr. We will continue without this table.
                    } else {
                        rerr = y.Wrapf(err, "Opening table: %q", fname)
                    }
                    return
                }

                mu.Lock()
                tables[tf.Level] = append(tables[tf.Level], t)
                mu.Unlock()
            }(fname, tf)
        }
        // å…³é—­ç›¸å…³çš„ä»»åŠ¡åç¨‹
        if err := throttle.Finish(); err != nil {
            closeAllTables(tables)
            return nil, err
        }
        db.opt.Infof("All %d tables opened in %s\n", atomic.LoadInt32(&numOpened),
            time.Since(start).Round(time.Millisecond))
        // è®°å½•å½“å‰fidæœ€å¤§å€¼
        s.nextFileID = maxFileID + 1
        // åˆå§‹åŒ–æ¯ä¸ªlevelçš„tables
        for i, tbls := range tables {
            s.levels[i].initTables(tbls)
        }

        // Make sure key ranges do not overlap etc.
        // å¿…è¦çš„æ•°æ®æ ¡éªŒ
        if err := s.validate(); err != nil {
            _ = s.cleanupLevels()
            return nil, y.Wrap(err, "Level validation")
        }

        // Sync directory (because we have at least removed some files, or previously created the
        // manifest file).
        // æ‰‹åŠ¨è¿›è¡ŒåŒæ­¥åˆ·ç›˜
        if err := syncDir(db.opt.Dir); err != nil {
            _ = s.close()
            return nil, err
        }

        return s, nil
    }

##### åˆ›å»ºlevelHandler

`newLevelHandler(db, level)` å‡½æ•°

    func newLevelHandler(db *DB, level int) *levelHandler {
        return &levelHandler{
            level:    level,
            strLevel: fmt.Sprintf("l%d", level),
            db:       db,
        }
    }

##### åˆå§‹åŒ–tables

`initTables(tables)` æ–¹æ³•

    // initTables replaces s.tables with given tables. This is done during loading.
    func (s *levelHandler) initTables(tables []*table.Table) {
        // åŠ é”
        s.Lock()
        defer s.Unlock()

        // èµ‹å€¼ä¸ç›¸å…³å€¼çš„åˆå§‹åŒ–
        s.tables = tables
        s.totalSize = 0
        s.totalStaleSize = 0
        for _, t := range tables {
            s.addSize(t)
        }
        // å¦‚æœæ˜¯L0å±‚,éœ€è¦æ‹¿æ¯ä¸ªfidæ’åº
        if s.level == 0 {
            // Key range will overlap. Just sort by fileID in ascending order
            // because newer tables are at the end of level 0.
            sort.Slice(s.tables, func(i, j int) bool {
                return s.tables[i].ID() < s.tables[j].ID()
            })
        } else {
            // L0å±‚å¾€ä¸Š,æ‹¿æ¯ä¸ªtableæ–‡ä»¶çš„MinKeyæ’åº
            // Sort tables by keys.
            sort.Slice(s.tables, func(i, j int) bool {
                return y.CompareKeys(s.tables[i].Smallest(), s.tables[j].Smallest()) < 0
            })
        }
    }

#### åˆå§‹åŒ–vlog

`init(db)` æ–¹æ³•

    // init initializes the value log struct. This initialization needs to happen
    // before compactions start.
    func (vlog *valueLog) init(db *DB) {
        // åŠ è½½é…ç½®
        vlog.opt = db.opt
        vlog.db = db
        // We don't need to open any vlog files or collect stats for GC if DB is opened
        // in InMemory mode. InMemory mode doesn't create any files/directories on disk.

        // inmem,é‚£æˆ‘èµ°?ğŸ¤¡
        if vlog.opt.InMemory {
            return
        }
        // æŒ‡å®šçš„vlogç›®å½•
        vlog.dirPath = vlog.opt.ValueDir
        // GCæ¨¡å—ç”¨åˆ°çš„channel
        vlog.garbageCh = make(chan struct{}, 1) // Only allow one GC at a time.
        // åˆ›å»ºä¸€ä¸ªGCæ¨¡å—ç›¸å…³æ–‡ä»¶
        lf, err := InitDiscardStats(vlog.opt)
        y.Check(err)
        vlog.discardStats = lf
    }

#### æ‰“å¼€vlog

`open(db)` æ–¹æ³•

    func (vlog *valueLog) open(db *DB) error {
        // We don't need to open any vlog files or collect stats for GC if DB is opened
        // in InMemory mode. InMemory mode doesn't create any files/directories on disk.
        // ä¸æƒ³å†åšè§£é‡Šäº†,inmem,é‚£æˆ‘èµ°!!!
        if db.opt.InMemory {
            return nil
        }
        // å¡«å……æ–‡ä»¶fidåˆ°filesMap
        if err := vlog.populateFilesMap(); err != nil {
            return err
        }
        // If no files are found, then create a new file.
        // å¦‚æœæ²¡æœ‰.vlogæ–‡ä»¶
        if len(vlog.filesMap) == 0 {
            if vlog.opt.ReadOnly {
                return nil
            }
            // åˆ›å»ºä¸€ä¸ª.vlogæ–‡ä»¶
            _, err := vlog.createVlogFile()
            return y.Wrapf(err, "Error while creating log file in valueLog.open")
        }
        fids := vlog.sortedFids()
        for _, fid := range fids {
            lf, ok := vlog.filesMap[fid]
            y.AssertTrue(ok)

            // Just open in RDWR mode. This should not create a new log file.
            lf.opt = vlog.opt
            if err := lf.open(vlog.fpath(fid), os.O_RDWR,
                2*vlog.opt.ValueLogFileSize); err != nil {
                return y.Wrapf(err, "Open existing file: %q", lf.path)
            }
            // We shouldn't delete the maxFid file.
            if lf.size == vlogHeaderSize && fid != vlog.maxFid {
                vlog.opt.Infof("Deleting empty file: %s", lf.path)
                if err := lf.Delete(); err != nil {
                    return y.Wrapf(err, "while trying to delete empty file: %s", lf.path)
                }
                delete(vlog.filesMap, fid)
            }
        }

        if vlog.opt.ReadOnly {
            return nil
        }
        // Now we can read the latest value log file, and see if it needs truncation. We could
        // technically do this over all the value log files, but that would mean slowing down the value
        // log open.
        last, ok := vlog.filesMap[vlog.maxFid]
        y.AssertTrue(ok)
        lastOff, err := last.iterate(vlog.opt.ReadOnly, vlogHeaderSize,
            func(_ Entry, vp valuePointer) error {
                return nil
            })
        if err != nil {
            return y.Wrapf(err, "while iterating over: %s", last.path)
        }
        if err := last.Truncate(int64(lastOff)); err != nil {
            return y.Wrapf(err, "while truncating last value log file: %s", last.path)
        }

        // Don't write to the old log file. Always create a new one.
        if _, err := vlog.createVlogFile(); err != nil {
            return y.Wrapf(err, "Error while creating log file in valueLog.open")
        }
        return nil
    }

`populateFilesMap()` æ–¹æ³•

    func (vlog *valueLog) populateFilesMap() error {
        vlog.filesMap = make(map[uint32]*logFile)

        // ä»ç›®å½•ä¸­æ‹¿åˆ°æ¯ä¸ªæ–‡ä»¶çš„å¥æŸ„
        files, _ := ioutil.ReadDir(vlog.dirPath)

        found := make(map[uint64]struct{})
        for _, file := range files {
            // åˆ¤æ–­æ˜¯å¦ä»¥.vlogä½œä¸ºåç¼€
            if !strings.HasSuffix(file.Name(), ".vlog") {
                continue
            }
            // å¯¹.vlogæ–‡ä»¶è¿›è¡Œæ ¡éªŒ,å»é™¤fid,è¿›è¡Œæ¶ˆé‡åˆ¤æ–­
            fsz := len(file.Name())
            fid, err := strconv.ParseUint(file.Name()[:fsz-5], 10, 32)
            if err != nil {
                return errFile(err, file.Name(), "Unable to parse log id.")
            }
            if _, ok := found[fid]; ok {
                return errFile(err, file.Name(), "Duplicate file found. Please delete one.")
            }
            found[fid] = struct{}{}

            lf := &logFile{
                fid:      uint32(fid),
                path:     vlog.fpath(uint32(fid)),
                registry: vlog.db.registry,
            }
            // æœ€åä¿å­˜åˆ°vlogçš„filesMapå½“ä¸­
            vlog.filesMap[uint32(fid)] = lf
            if vlog.maxFid < uint32(fid) {
                vlog.maxFid = uint32(fid)
            }
        }
        // ç›´åˆ°æ¯ä¸ª.vlogæ–‡ä»¶çš„fidéƒ½æ·»åŠ åˆ°äº†mapä¸­
        // ç¬¬ä¸€æ¬¡åˆå§‹åŒ–æ—¶æ²¡æœ‰.vlogæ–‡ä»¶,æ•…ç›´æ¥è·³è¿‡
        return nil
    }

##### åˆ›å»ºvlogæ–‡ä»¶

`createVlogFile()`æ–¹æ³•

    func (vlog *valueLog) createVlogFile() (*logFile, error) {
        // æœ€å¤§çš„fid
        fid := vlog.maxFid + 1
        // æ ¹æ®fidå‘½å
        path := vlog.fpath(fid)
        // åˆ›å»ºä¸€ä¸ªå¥æŸ„å®ä¾‹
        lf := &logFile{
            fid:      fid,
            path:     path,
            registry: vlog.db.registry,
            writeAt:  vlogHeaderSize,
            opt:      vlog.opt,
        }
        // è¿›è¡Œç³»ç»Ÿè°ƒç”¨æ‰“å¼€æ–‡ä»¶,é€šè¿‡mmapçš„æ–¹å¼
        // .vlogæ–‡ä»¶åˆå§‹åŒ–æ—¶ä¼šåˆ›å»ºä¸€ä¸ª2Gçš„æ–‡ä»¶
        err := lf.open(path, os.O_RDWR'os.O_CREATE'os.O_EXCL, 2*vlog.opt.ValueLogFileSize)
        if err != z.NewFile && err != nil {
            return nil, err
        }

        // è¿›è¡Œæ•°æ®åˆå§‹åŒ–æ›´æ–°çš„æ“ä½œ
        vlog.filesLock.Lock()
        vlog.filesMap[fid] = lf
        y.AssertTrue(vlog.maxFid < fid)
        vlog.maxFid = fid
        // writableLogOffset is only written by write func, by read by Read func.
        // To avoid a race condition, all reads and updates to this variable must be
        // done via atomics.
        atomic.StoreUint32(&vlog.writableLogOffset, vlogHeaderSize)
        vlog.numEntriesWritten = 0
        vlog.filesLock.Unlock()

        return lf, nil
    }
    